{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3qXNhSWUxpU",
    "colab_type": "text"
   },
   "source": [
    "# Data presentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJlv2Rq6WbYj",
    "colab_type": "text"
   },
   "source": [
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab_type": "code",
    "id": "BCZbsyRS5Xs2",
    "outputId": "2c589ea1-2400-4412-af9b-0199ba170873",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.575018078986E12,
     "user_tz": -60.0,
     "elapsed": 3714.0,
     "user": {
      "displayName": "THAY Daniel",
      "photoUrl": "",
      "userId": "08618153734985027049"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-11-29 09:01:16--  https://marwachafii.github.io/assets/datasets/Sarcasm_Headlines_Dataset.json\n",
      "Resolving marwachafii.github.io (marwachafii.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
      "Connecting to marwachafii.github.io (marwachafii.github.io)|185.199.108.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5616833 (5.4M) [application/json]\n",
      "Saving to: ‘Sarcasm_Headlines_Dataset.json.6’\n",
      "\n",
      "\r          Sarcasm_H   0%[                    ]       0  --.-KB/s               \rSarcasm_Headlines_D 100%[===================>]   5.36M  --.-KB/s    in 0.05s   \n",
      "\n",
      "2019-11-29 09:01:16 (103 MB/s) - ‘Sarcasm_Headlines_Dataset.json.6’ saved [5616833/5616833]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://marwachafii.github.io/assets/datasets/Sarcasm_Headlines_Dataset.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVFqZBIkWtmv",
    "colab_type": "text"
   },
   "source": [
    "## Exploring the data\n",
    "\n",
    "The downloaded file is in the [JSON](https://en.wikipedia.org/wiki/JSON) format.\n",
    "Usually, a json file stores a single dictionary (a json object) or a list of dictionaries (a json array).\n",
    "\n",
    "In order to read the file we will use the Python package `json` and then check the type of the loaded content (list or dict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "e7MXNdNZXinn",
    "colab_type": "code",
    "outputId": "7363f5b0-0dfd-4ab4-e11d-0247beae9a14",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.575018078989E12,
     "user_tz": -60.0,
     "elapsed": 3694.0,
     "user": {
      "displayName": "THAY Daniel",
      "photoUrl": "",
      "userId": "08618153734985027049"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_name = \"Sarcasm_Headlines_Dataset.json\"\n",
    "with open(file_name) as f:\n",
    "  dataset = json.load(f)\n",
    "\n",
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPkOUt7UYa34",
    "colab_type": "text"
   },
   "source": [
    "Since our dataset is a list, let us display the first item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mZcPlGIoYvOq",
    "colab_type": "code",
    "outputId": "8686c2f2-e4ae-4337-e02e-1b4c73ef3f0c",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.57501807899E12,
     "user_tz": -60.0,
     "elapsed": 3673.0,
     "user": {
      "displayName": "THAY Daniel",
      "photoUrl": "",
      "userId": "08618153734985027049"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first item:\n",
      "\n",
      "{'article_link': 'https://www.huffingtonpost.com/entry/versace-black-code_us_5861fbefe4b0de3a08f600d5', 'headline': \"former versace store clerk sues over secret 'black code' for minority shoppers\", 'is_sarcastic': 0}\n",
      "\n",
      "\n",
      "The first item's keys:\n",
      "\n",
      "['article_link', 'headline', 'is_sarcastic']\n",
      "\n",
      "\n",
      "The first item's values:\n",
      "\n",
      "['https://www.huffingtonpost.com/entry/versace-black-code_us_5861fbefe4b0de3a08f600d5', \"former versace store clerk sues over secret 'black code' for minority shoppers\", 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"The first item:\\n\")\n",
    "print(dataset[0])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"The first item's keys:\\n\")\n",
    "print(list(dataset[0].keys()))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"The first item's values:\\n\")\n",
    "print(list(dataset[0].values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhZAj2jTY-i_",
    "colab_type": "text"
   },
   "source": [
    "When loading a list of dictionaries from a JSON file, you should not assume that all the list's dictionaries have the same keys.\n",
    "\n",
    "The following example is a valid JSON:\n",
    "\n",
    "```JSON\n",
    "[\n",
    "  {\n",
    "    \"a\": 123,\n",
    "    \"b\": \"test\"\n",
    "  },\n",
    "  {\n",
    "    \"d\": false,\n",
    "    \"b\": \"test\",\n",
    "    \"e\": 12.6\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "That being said, we will not be facing this issue with the dataset at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LraTn-2Ja6X0",
    "colab_type": "text"
   },
   "source": [
    "# Data preparation\n",
    "\n",
    "We will need two lists:\n",
    "- the first created from the `headline` value of every dataset item\n",
    "- the second created from the `is_sarcastic` value of every dataset item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "SmITzXiCbml5",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following code uses 'list comprehensions':\n",
    "https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions\n",
    "\"\"\"\n",
    "headlines = [item[\"headline\"] for item in dataset]\n",
    "labels = [item[\"is_sarcastic\"] for item in dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8o3iKNHcrU0",
    "colab_type": "text"
   },
   "source": [
    "Some basic stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code",
    "id": "HXFMSc7g6qy0",
    "outputId": "32890d87-2b3b-45ab-d208-7d4af0931b76",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.575018080223E12,
     "user_tz": -60.0,
     "elapsed": 4868.0,
     "user": {
      "displayName": "THAY Daniel",
      "photoUrl": "",
      "userId": "08618153734985027049"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98.0
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11724 of 26709 headlines are sarcastic (43.89531618555543%)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "print(f\"{np.sum(labels)} of {len(headlines)} headlines are sarcastic ({np.mean(labels)*100}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zz4_ul_Z8eCd"
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "First, we will build a dictionary that associates an integer with each word (for example, the 10k most frequent words). Remember to reserve an integer for non-encoded words (1 for example) and an integer for the absence of words (0 for example).\n",
    "\n",
    "Then we will convert a sentence into a sequence of integers using the dictionary. Then we will transform the sentences into fixed size vectors by zero padding.\n",
    "\n",
    "Example of mapping words to integers and a phrase to a vector:\n",
    "\n",
    "```\n",
    "Word dictionary\n",
    "\n",
    "new <--> 12356\n",
    "president <--> 756\n",
    "elected <--> 12374\n",
    "unknown words <--> 1\n",
    "\n",
    "a new president elected <--> [1, 12356, 756, 12374]\n",
    "\n",
    "Notice we mapped the word \"a\" to 1 since it was not found on our word dictionary.\n",
    "\n",
    "In order to have same size vectors and if for example our longuest sentence is 9 words long, we can add 5 zeroes (zero padding) at the end of our vector (since our vector has only 5 items in it).\n",
    "\n",
    "a new president elected <--> [1, 12356, 756, 12374, 0, 0, 0, 0, 0]\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code",
    "id": "hSLaCHDq8Q7i",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can proceed as follows (you are free to do otherwise):\n",
    "\n",
    "- Create word dictionary from the headlines\n",
    "- Select the 10000 most recurrent words\n",
    "- Create a dictionary that maps every word from the 10000 list to a number\n",
    "  0 is reserved for padding vectors and 1 for words not found in the 10000\n",
    "  word list\n",
    "\"\"\"\n",
    "\n",
    "#On crée la liste contenant tous les mots de toutes les headlines (phrases) . \n",
    "headlines = [item[\"headline\"] for item in dataset]\n",
    "word_list=[]\n",
    "for sentence in headlines:\n",
    "  words=keras.preprocessing.text.text_to_word_sequence(sentence, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ')\n",
    "  for word in words:\n",
    "    word_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "w5XhCB09InXy",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "#On crée une liste contenant tous les mots sans répétition\n",
    "list=[]\n",
    "for word in word_list:\n",
    "  if word not in list:\n",
    "    list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "b6vwkUe_InJs",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "#On crée une liste qui contient l'occurence de tous ces mots\n",
    "list_count=[]\n",
    "for word in list:\n",
    "  count=word_list.count(word)\n",
    "  list_count.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "ZUyZa9tzL9sH",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "#On crée la liste contenant les 10000 mots les plus récurrents\n",
    "most_recurrent_words=[]\n",
    "for i in range(10000):\n",
    "  maxcount=max(list_count)\n",
    "  indice=list_count.index(maxcount)\n",
    "  most_recurrent_words.append(list[indice])\n",
    "  list_count.remove(maxcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Krbzls-1YgQd",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "aa92b4c1-78cd-448a-9560-95897b13a915",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.57501910096E12,
     "user_tz": -60.0,
     "elapsed": 91439.0,
     "user": {
      "displayName": "THAY Daniel",
      "photoUrl": "",
      "userId": "08618153734985027049"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to\n"
     ]
    }
   ],
   "source": [
    "print(most_recurrent_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "szG8p91cObAu",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "#On crée le dictionnaire des 10000 mots les plus récurrents \n",
    "word_to_int = []\n",
    "for word in most_recurrent_words:\n",
    "  word_to_int.append(most_reccurent_words.index(word)+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "0q0ey-NaQXmA",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "#Longueur de la phrase la plus longue pour le zero-padding\n",
    "len_sentence=[]\n",
    "for sentence in headlines:\n",
    "  words=keras.preprocessing.text.text_to_word_sequence(sentence, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ')\n",
    "  len_sentence.append(len(words))\n",
    "max_len_sentence=max(len_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "iTgAaD3-Rrc5",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "#On convertit toutes les phrases en liste d'integer de longueur max_len_sentence à l'aide du dictionnaire.\n",
    "list_vector=[]\n",
    "for sentence in headlines:\n",
    "  words=keras.preprocessing.text.text_to_word_sequence(sentence, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ')\n",
    "  vector=np.zeros(max_len_sentence)\n",
    "  for word in words:\n",
    "    index=words.index(word)\n",
    "    if word not in most_reccurent_words:\n",
    "      vector[index]=1\n",
    "    else:\n",
    "      vector[index]=word_to_int[most_recurrent_words.index(word)]\n",
    "  list_vector.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "rOH2__zwdnax",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121.0
    },
    "outputId": "6a055030-3087-4874-8f30-1c7e052f81a2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.575019834754E12,
     "user_tz": -60.0,
     "elapsed": 969.0,
     "user": {
      "displayName": "THAY Daniel",
      "photoUrl": "",
      "userId": "08618153734985027049"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.080e+02 6.790e+02 2.298e+03 2.577e+03 3.820e+02 4.800e+01 2.746e+03\n",
      " 3.112e+03 6.970e+03 6.000e+00 2.924e+03 5.274e+03 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(list_vector[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_9osKOJb-H5B"
   },
   "source": [
    "# Classification\n",
    "\n",
    "**Deep neural network**\n",
    "\n",
    "We will now create a network of neurons that takes a whole vector of indices (an encoded sentence) and outputs a prediction: \"sarcasm\" or \"no sarcasm\"\n",
    "\n",
    "Guidelines:\n",
    "\n",
    "- Remember to divide your set into train, val and test subsets\n",
    "- The first layer of our network is an embedding layer (use `tf.keras.layers.Embedding`)\n",
    "- Choose the hidden layers as you see fit\n",
    "- The last layer has only one output neuron with a sigmoid activation function corresponding to a probability\n",
    "- The applied cost function is a binary crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab_type": "code",
    "id": "d0qyOuQ9_Wny",
    "outputId": "689a73fd-3059-4399-ad71-074b8feb68ba",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.575023765684E12,
     "user_tz": -60.0,
     "elapsed": 964.0,
     "user": {
      "displayName": "THAY Daniel",
      "photoUrl": "",
      "userId": "08618153734985027049"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 40, 64)            640064    \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 2561      \n",
      "=================================================================\n",
      "Total params: 642,625\n",
      "Trainable params: 642,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#On crée notre premier modèle \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Embedding, SimpleRNN, LSTM, Conv1D\n",
    "\n",
    "X_train=list_vector[0:20710] \n",
    "X_val=list_vector[20710:23710]\n",
    "X_test=list_vector[23710:26710]\n",
    "Y_train=labels[0:20710]\n",
    "Y_val=labels[20710:23710]\n",
    "Y_test=labels[23710:26710]\n",
    "\n",
    "X_train = np.asarray(X_train)\n",
    "X_val = np.asarray(X_val )\n",
    "X_test = np.asarray(X_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10001, 64, input_length=max_len_sentence))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "y9EVIE9PlPtt",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000.0
    },
    "outputId": "5e492063-7ba5-4400-9ba3-229a809acef1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.575021643219E12,
     "user_tz": -60.0,
     "elapsed": 76654.0,
     "user": {
      "displayName": "THAY Daniel",
      "photoUrl": "",
      "userId": "08618153734985027049"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 20710 samples, validate on 3000 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "20710/20710 [==============================] - 9s 426us/step - loss: 0.4946 - acc: 0.7519 - val_loss: 0.3883 - val_acc: 0.8157\n",
      "Epoch 2/20\n",
      "20710/20710 [==============================] - 4s 176us/step - loss: 0.3039 - acc: 0.8727 - val_loss: 0.3695 - val_acc: 0.8233\n",
      "Epoch 3/20\n",
      "20710/20710 [==============================] - 4s 170us/step - loss: 0.2306 - acc: 0.9080 - val_loss: 0.3822 - val_acc: 0.8243\n",
      "Epoch 4/20\n",
      "20710/20710 [==============================] - 3s 165us/step - loss: 0.1758 - acc: 0.9358 - val_loss: 0.4094 - val_acc: 0.8243\n",
      "Epoch 5/20\n",
      "20710/20710 [==============================] - 3s 165us/step - loss: 0.1343 - acc: 0.9549 - val_loss: 0.4399 - val_acc: 0.8180\n",
      "Epoch 6/20\n",
      "20710/20710 [==============================] - 3s 163us/step - loss: 0.1023 - acc: 0.9679 - val_loss: 0.4809 - val_acc: 0.8157\n",
      "Epoch 7/20\n",
      "20710/20710 [==============================] - 4s 171us/step - loss: 0.0788 - acc: 0.9772 - val_loss: 0.5207 - val_acc: 0.8103\n",
      "Epoch 8/20\n",
      "20710/20710 [==============================] - 3s 166us/step - loss: 0.0620 - acc: 0.9826 - val_loss: 0.5611 - val_acc: 0.8100\n",
      "Epoch 9/20\n",
      "20710/20710 [==============================] - 3s 163us/step - loss: 0.0493 - acc: 0.9868 - val_loss: 0.6137 - val_acc: 0.8067\n",
      "Epoch 10/20\n",
      "20710/20710 [==============================] - 3s 159us/step - loss: 0.0405 - acc: 0.9889 - val_loss: 0.6642 - val_acc: 0.8047\n",
      "Epoch 11/20\n",
      "20710/20710 [==============================] - 3s 165us/step - loss: 0.0334 - acc: 0.9906 - val_loss: 0.7096 - val_acc: 0.8007\n",
      "Epoch 12/20\n",
      "20710/20710 [==============================] - 3s 164us/step - loss: 0.0286 - acc: 0.9918 - val_loss: 0.7611 - val_acc: 0.7950\n",
      "Epoch 13/20\n",
      "20710/20710 [==============================] - 3s 164us/step - loss: 0.0248 - acc: 0.9923 - val_loss: 0.7992 - val_acc: 0.7963\n",
      "Epoch 14/20\n",
      "20710/20710 [==============================] - 3s 162us/step - loss: 0.0216 - acc: 0.9932 - val_loss: 0.8517 - val_acc: 0.7967\n",
      "Epoch 15/20\n",
      "20710/20710 [==============================] - 4s 176us/step - loss: 0.0190 - acc: 0.9939 - val_loss: 0.9030 - val_acc: 0.7927\n",
      "Epoch 16/20\n",
      "20710/20710 [==============================] - 4s 175us/step - loss: 0.0168 - acc: 0.9944 - val_loss: 0.9627 - val_acc: 0.7903\n",
      "Epoch 17/20\n",
      "20710/20710 [==============================] - 4s 176us/step - loss: 0.0155 - acc: 0.9948 - val_loss: 1.0082 - val_acc: 0.7900\n",
      "Epoch 18/20\n",
      "20710/20710 [==============================] - 4s 178us/step - loss: 0.0143 - acc: 0.9949 - val_loss: 1.0319 - val_acc: 0.7927\n",
      "Epoch 19/20\n",
      "20710/20710 [==============================] - 4s 177us/step - loss: 0.0134 - acc: 0.9951 - val_loss: 1.0853 - val_acc: 0.7883\n",
      "Epoch 20/20\n",
      "20710/20710 [==============================] - 4s 177us/step - loss: 0.0125 - acc: 0.9955 - val_loss: 1.1487 - val_acc: 0.7877\n",
      "Test loss: 1.1471892492836337\n",
      "Test accuracy: 0.7905968657808726\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 20\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, Y_val))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOD6F9y1nCAr",
    "colab_type": "text"
   },
   "source": [
    "On obtient une précision de test de 0.79, ce qui est un bon résultat. Cependant la précision de training est de 0.99, on est donc dans un cas d'overfitting. Pour cela on peut diminuer le nombre d'epoch (early-stopping) ou bien augmenter le dataset d'entraînement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "9hd46doXpAq6",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503.0
    },
    "outputId": "4cdee75b-6f25-4d3f-d502-becd4689eb21",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.575022480163E12,
     "user_tz": -60.0,
     "elapsed": 18930.0,
     "user": {
      "displayName": "THAY Daniel",
      "photoUrl": "",
      "userId": "08618153734985027049"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 40, 64)            640064    \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 2561      \n",
      "=================================================================\n",
      "Total params: 642,625\n",
      "Trainable params: 642,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 21368 samples, validate on 3342 samples\n",
      "Epoch 1/5\n",
      "21368/21368 [==============================] - 4s 179us/step - loss: 0.4829 - acc: 0.7624 - val_loss: 0.3733 - val_acc: 0.8208\n",
      "Epoch 2/5\n",
      "21368/21368 [==============================] - 4s 168us/step - loss: 0.3036 - acc: 0.8733 - val_loss: 0.3554 - val_acc: 0.8294\n",
      "Epoch 3/5\n",
      "21368/21368 [==============================] - 3s 162us/step - loss: 0.2324 - acc: 0.9078 - val_loss: 0.3667 - val_acc: 0.8285\n",
      "Epoch 4/5\n",
      "21368/21368 [==============================] - 3s 160us/step - loss: 0.1777 - acc: 0.9352 - val_loss: 0.3863 - val_acc: 0.8268\n",
      "Epoch 5/5\n",
      "21368/21368 [==============================] - 3s 163us/step - loss: 0.1363 - acc: 0.9522 - val_loss: 0.4173 - val_acc: 0.8241\n",
      "Test loss: 0.4288494803268591\n",
      "Test accuracy: 0.819909955007306\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train=list_vector[0:21368] \n",
    "X_val=list_vector[21368:24710]\n",
    "X_test=list_vector[24710:26710]\n",
    "Y_train=labels[0:21368]\n",
    "Y_val=labels[21368:24710]\n",
    "Y_test=labels[24710:26710]\n",
    "\n",
    "X_train = np.asarray(X_train)\n",
    "X_val = np.asarray(X_val )\n",
    "X_test = np.asarray(X_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10001, 64, input_length=max_len_sentence))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, Y_val))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFJgIBvjqIbA",
    "colab_type": "text"
   },
   "source": [
    "On obtient des résultats légèrement meilleurs.\n",
    "On tente d'améliorer notre modèle en utilisant une couche RNN (afin d'ajouter une dépendance entre les mots d'une phrase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code",
    "outputId": "6556c9ef-14d8-40e3-fb34-53ad647dc5da",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.575024112255E12,
     "user_tz": -60.0,
     "elapsed": 255829.0,
     "user": {
      "displayName": "THAY Daniel",
      "photoUrl": "",
      "userId": "08618153734985027049"
     }
    },
    "id": "7Cn2IJRNy8Cc",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 40, 64)            640064    \n",
      "_________________________________________________________________\n",
      "simple_rnn_3 (SimpleRNN)     (None, 32)                3104      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 643,201\n",
      "Trainable params: 643,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 21368 samples, validate on 3342 samples\n",
      "Epoch 1/10\n",
      "21368/21368 [==============================] - 27s 1ms/step - loss: 0.4855 - acc: 0.7642 - val_loss: 0.4233 - val_acc: 0.8172\n",
      "Epoch 2/10\n",
      "21368/21368 [==============================] - 25s 1ms/step - loss: 0.3254 - acc: 0.8658 - val_loss: 0.4246 - val_acc: 0.8073\n",
      "Epoch 3/10\n",
      "21368/21368 [==============================] - 25s 1ms/step - loss: 0.2515 - acc: 0.8993 - val_loss: 0.4392 - val_acc: 0.8088\n",
      "Epoch 4/10\n",
      "21368/21368 [==============================] - 25s 1ms/step - loss: 0.2056 - acc: 0.9195 - val_loss: 0.5197 - val_acc: 0.8085\n",
      "Epoch 5/10\n",
      "21368/21368 [==============================] - 26s 1ms/step - loss: 0.1779 - acc: 0.9310 - val_loss: 0.5219 - val_acc: 0.7971\n",
      "Epoch 6/10\n",
      "21368/21368 [==============================] - 25s 1ms/step - loss: 0.1521 - acc: 0.9437 - val_loss: 0.5529 - val_acc: 0.8055\n",
      "Epoch 7/10\n",
      "21368/21368 [==============================] - 25s 1ms/step - loss: 0.1334 - acc: 0.9518 - val_loss: 0.6033 - val_acc: 0.7843\n",
      "Epoch 8/10\n",
      "21368/21368 [==============================] - 25s 1ms/step - loss: 0.1213 - acc: 0.9560 - val_loss: 0.6133 - val_acc: 0.7926\n",
      "Epoch 9/10\n",
      "21368/21368 [==============================] - 25s 1ms/step - loss: 0.1045 - acc: 0.9630 - val_loss: 0.6928 - val_acc: 0.7914\n",
      "Epoch 10/10\n",
      "21368/21368 [==============================] - 25s 1ms/step - loss: 0.0912 - acc: 0.9697 - val_loss: 0.7556 - val_acc: 0.8049\n",
      "Test loss: 0.755467976269691\n",
      "Test accuracy: 0.800900450284747\n"
     ]
    }
   ],
   "source": [
    "X_train=list_vector[0:21368] \n",
    "X_val=list_vector[21368:24710]\n",
    "X_test=list_vector[24710:26710]\n",
    "Y_train=labels[0:21368]\n",
    "Y_val=labels[21368:24710]\n",
    "Y_test=labels[24710:26710]\n",
    "\n",
    "X_train = np.asarray(X_train)\n",
    "X_val = np.asarray(X_val )\n",
    "X_test = np.asarray(X_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10001, 64, input_length=max_len_sentence))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, Y_val))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpOh-zZizR1v",
    "colab_type": "text"
   },
   "source": [
    "Les résultats ne sont pas vraiment meilleurs. \n",
    "On utilise à la place une couche LSTM ( même principe que le RNN mais qui permet de relier plus efficacement deux mots éloignés d'une phrase (car pour le RNN il y a le problème du vanishing gradient))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "jYTAIuePuHZY",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503.0
    },
    "outputId": "c91a0c38-67bf-4003-c395-079de8fceb36",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.575024435578E12,
     "user_tz": -60.0,
     "elapsed": 310150.0,
     "user": {
      "displayName": "THAY Daniel",
      "photoUrl": "",
      "userId": "08618153734985027049"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, 40, 64)            640064    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 652,513\n",
      "Trainable params: 652,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 21368 samples, validate on 3342 samples\n",
      "Epoch 1/5\n",
      "21368/21368 [==============================] - 63s 3ms/step - loss: 0.5635 - acc: 0.7004 - val_loss: 0.4429 - val_acc: 0.8001\n",
      "Epoch 2/5\n",
      "21368/21368 [==============================] - 61s 3ms/step - loss: 0.3652 - acc: 0.8422 - val_loss: 0.4078 - val_acc: 0.8142\n",
      "Epoch 3/5\n",
      "21368/21368 [==============================] - 61s 3ms/step - loss: 0.3114 - acc: 0.8667 - val_loss: 0.4416 - val_acc: 0.8082\n",
      "Epoch 4/5\n",
      "21368/21368 [==============================] - 61s 3ms/step - loss: 0.2726 - acc: 0.8771 - val_loss: 0.4695 - val_acc: 0.8139\n",
      "Epoch 5/5\n",
      "21368/21368 [==============================] - 61s 3ms/step - loss: 0.2437 - acc: 0.8895 - val_loss: 0.4830 - val_acc: 0.8148\n",
      "Test loss: 0.5095036876923207\n",
      "Test accuracy: 0.80040020010005\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10001, 64, input_length=max_len_sentence))\n",
    "model.add(LSTM(32)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 32\n",
    "epochs = \n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, Y_val))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpXp0ibhz4G_",
    "colab_type": "text"
   },
   "source": [
    "On a beaucoup moins d'overfitting, la précision de test est plutot bonne par rapport à la précision d'entraînement. On pourrait obtenir de meilleurs résultats avec plus d'epochs. On rajoute une couche Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "rvK-azFDuP2M",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538.0
    },
    "outputId": "473589c5-4da8-40b7-c972-2b3ce7a3986b",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.575024741786E12,
     "user_tz": -60.0,
     "elapsed": 295655.0,
     "user": {
      "displayName": "THAY Daniel",
      "photoUrl": "",
      "userId": "08618153734985027049"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 40, 64)            640064    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 36, 64)            20544     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 673,057\n",
      "Trainable params: 673,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 21368 samples, validate on 3342 samples\n",
      "Epoch 1/5\n",
      "21368/21368 [==============================] - 62s 3ms/step - loss: 0.6680 - acc: 0.5942 - val_loss: 0.6480 - val_acc: 0.6397\n",
      "Epoch 2/5\n",
      "21368/21368 [==============================] - 57s 3ms/step - loss: 0.6792 - acc: 0.5788 - val_loss: 0.6805 - val_acc: 0.5787\n",
      "Epoch 3/5\n",
      "21368/21368 [==============================] - 57s 3ms/step - loss: 0.6349 - acc: 0.6307 - val_loss: 0.5239 - val_acc: 0.7627\n",
      "Epoch 4/5\n",
      "21368/21368 [==============================] - 57s 3ms/step - loss: 0.4094 - acc: 0.8190 - val_loss: 0.4170 - val_acc: 0.8031\n",
      "Epoch 5/5\n",
      "21368/21368 [==============================] - 57s 3ms/step - loss: 0.3030 - acc: 0.8730 - val_loss: 0.4182 - val_acc: 0.8163\n",
      "Test loss: 0.42554853333271164\n",
      "Test accuracy: 0.8139069534767384\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10001, 64, input_length=max_len_sentence))\n",
    "model.add(Conv1D(64,5,strides=1))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, Y_val))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giy3pVQx0XFN",
    "colab_type": "text"
   },
   "source": [
    "En rajoutant une couche Conv1D, on remarque une petite amélioration au niveau de la précision tout en n'étant pas en overfitting ou underfitting.\n",
    "\n",
    "On s'attendait à avoir de meilleurs résultats en utilisant le LSTM mais cela est sûrement dû à la base de données initiale (peut etre que l'on avait besoin d'entrainer avec toute la data set afin de traiter tous les cas et utiliser d'autres exemples pour les dataset de tests et validation). "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "news_sarcasm.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
